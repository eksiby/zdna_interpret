{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "accelerate                1.6.0\n",
      "anyio                     4.9.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.3.0\n",
      "babel                     2.17.0\n",
      "backcall                  0.2.0\n",
      "beautifulsoup4            4.13.4\n",
      "biopython                 1.85\n",
      "bleach                    6.2.0\n",
      "captum                    0.8.0\n",
      "certifi                   2025.1.31\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.1\n",
      "cloudpickle               3.1.1\n",
      "comm                      0.2.2\n",
      "contourpy                 1.3.0\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.14\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "exceptiongroup            1.2.2\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.18.0\n",
      "fonttools                 4.57.0\n",
      "fqdn                      1.5.1\n",
      "fsspec                    2025.3.2\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.8\n",
      "httpx                     0.28.1\n",
      "huggingface-hub           0.30.2\n",
      "idna                      3.10\n",
      "importlib_metadata        8.6.1\n",
      "importlib_resources       6.5.2\n",
      "ipykernel                 6.29.5\n",
      "ipython                   7.34.0\n",
      "ipywidgets                8.1.6\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "joblib                    1.4.2\n",
      "json5                     0.12.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.15.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.0\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.14\n",
      "kiwisolver                1.4.7\n",
      "llvmlite                  0.43.0\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib                3.9.4\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.3\n",
      "mpmath                    1.3.0\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.2.1\n",
      "notebook                  7.4.0\n",
      "notebook_shim             0.2.4\n",
      "numba                     0.60.0\n",
      "numpy                     1.25.0\n",
      "nvidia-cublas-cu12        12.4.5.8\n",
      "nvidia-cuda-cupti-cu12    12.4.127\n",
      "nvidia-cuda-nvrtc-cu12    12.4.127\n",
      "nvidia-cuda-runtime-cu12  12.4.127\n",
      "nvidia-cudnn-cu12         9.1.0.70\n",
      "nvidia-cufft-cu12         11.2.1.3\n",
      "nvidia-curand-cu12        10.3.5.147\n",
      "nvidia-cusolver-cu12      11.6.1.9\n",
      "nvidia-cusparse-cu12      12.3.1.170\n",
      "nvidia-cusparselt-cu12    0.6.2\n",
      "nvidia-nccl-cu12          2.21.5\n",
      "nvidia-nvjitlink-cu12     12.4.127\n",
      "nvidia-nvtx-cu12          12.4.127\n",
      "overrides                 7.7.0\n",
      "packaging                 25.0\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "peft                      0.15.2\n",
      "pexpect                   4.9.0\n",
      "pickleshare               0.7.5\n",
      "pillow                    11.2.1\n",
      "pip                       25.0\n",
      "platformdirs              4.3.7\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.51\n",
      "psutil                    7.0.0\n",
      "ptyprocess                0.7.0\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.1\n",
      "pyparsing                 3.2.3\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.3.0\n",
      "pytz                      2025.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.4.0\n",
      "referencing               0.36.2\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.24.0\n",
      "safetensors               0.5.3\n",
      "scikit-learn              1.6.1\n",
      "scipy                     1.13.1\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.8.0\n",
      "shap                      0.47.2\n",
      "six                       1.17.0\n",
      "slicer                    0.0.8\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.7\n",
      "sympy                     1.13.1\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.6.0\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.21.1\n",
      "tomli                     2.2.1\n",
      "torch                     2.6.0\n",
      "tornado                   6.4.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "transformers              4.51.3\n",
      "transformers-interpret    0.10.0\n",
      "triton                    3.2.0\n",
      "types-python-dateutil     2.9.0.20241206\n",
      "typing_extensions         4.13.2\n",
      "tzdata                    2025.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.4.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wheel                     0.45.1\n",
      "widgetsnbextension        4.0.14\n",
      "zipp                      3.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "from Sparse_vector import SparseVector\n",
    "from scipy.signal import convolve2d, convolve\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!git clone https://github.com/Nazar1997/Sparse_vector.git \n",
    "!git clone https://github.com/vladislareon/z_dna'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ASSEMBLY_d = {}\n",
    "chroms_d = {}\n",
    "all_features_d = {}\n",
    "groups_d = {}\n",
    "feature_names_d = {}\n",
    "ZDNA_d = {}\n",
    "black_list_d = {}\n",
    "DNA_d = {}\n",
    "DNA_features_d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ASSEMBLY = \"curax_14h_UNI_mm9\"\n",
    "chroms = [f'chr{i}' for i in list(range(1, 20)) + ['X', 'Y']]\n",
    "all_features = sorted([i[:-4] for i in os.listdir('../data/mm9_features/sparse/') if i.endswith('.pkl')])\n",
    "groups = ['DNase-seq', 'Histone', 'RNA polymerase', 'TFs and others']\n",
    "feature_names = [i for i in all_features if (i.split('_')[0] in groups)]\n",
    "ZDNA = load(f'../data/mm9_zdna/sparse/{ASSEMBLY}.pkl')\n",
    "black_list = load(f'../data/mm9_zdna/sparse/blacklist_mm9.pkl')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DNA = {chrom:load(f'../data/mm9_dna/sparse/{chrom}.pkl') for chrom in tqdm(chroms)}\n",
    "\n",
    "DNA_features = {feture: load(f'../data/mm9_features/sparse/{feture}.pkl')\n",
    "                for feture in tqdm(feature_names)}\n",
    "\n",
    "for feature in tqdm(DNA_features):\n",
    "    if set(DNA_features[feature].keys()) != set(chroms):\n",
    "        for chrom in chroms:\n",
    "            if chrom not in DNA_features[feature]:\n",
    "                DNA_features[feature][chrom] = SparseVector(len(DNA[chrom]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mode = 'mm9'\n",
    "ASSEMBLY_d[mode] = ASSEMBLY\n",
    "chroms_d[mode] = chroms\n",
    "all_features_d[mode] = all_features\n",
    "groups_d[mode] = groups\n",
    "feature_names_d[mode] = feature_names\n",
    "ZDNA_d[mode] = ZDNA\n",
    "black_list_d[mode] = black_list\n",
    "DNA_d[mode] = DNA\n",
    "DNA_features_d[mode] = DNA_features'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ASSEMBLY = \"ZDNA_2016\"\n",
    "chroms = [f'chr{i}' for i in list(range(1, 23)) + ['X', 'Y']]\n",
    "all_features = sorted([i[:-4] for i in os.listdir('z_dna/hg38_features/sparse/') if i.endswith('.pkl')])\n",
    "groups = ['DNase-seq', 'Histone', 'RNA polymerase', 'TFs and others']\n",
    "feature_names = [i for i in all_features if (i.split('_')[0] in groups)]\n",
    "ZDNA = load('ZDNA_cousine.pkl')\n",
    "black_list = load(f'blacklist_hg38_v2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:00<00:00, 1321.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "def chrom_reader(chrom):\n",
    "    files = sorted([i for i in os.listdir(f'z_dna/hg38_dna/') if f\"{chrom}_\" in i])\n",
    "    return ''.join([load(f\"z_dna/hg38_dna/{file}\") for file in files])\n",
    "\n",
    "\n",
    "DNA = {chrom:chrom_reader(chrom) for chrom in tqdm(chroms)}\n",
    "\n",
    "DNA_features = {feture: load(f'z_dna/hg38_features/sparse/{feture}.pkl')\n",
    "                for feture in tqdm(feature_names)}\n",
    "\n",
    "for feature in tqdm(DNA_features):\n",
    "    if set(DNA_features[feature].keys()) != set(chroms):\n",
    "        for chrom in chroms:\n",
    "            if chrom not in DNA_features[feature]:\n",
    "                DNA_features[feature][chrom] = SparseVector(len(DNA[chrom]))\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = 'hg38'\n",
    "ASSEMBLY_d[mode] = ASSEMBLY\n",
    "chroms_d[mode] = chroms\n",
    "all_features_d[mode] = all_features\n",
    "groups_d[mode] = groups\n",
    "feature_names_d[mode] = feature_names\n",
    "ZDNA_d[mode] = ZDNA\n",
    "black_list_d[mode] = black_list\n",
    "DNA_d[mode] = DNA\n",
    "DNA_features_d[mode] = DNA_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#intersect = {i.upper() for i in DNA_features_d['mm9']} & {i.upper() for i in DNA_features_d['hg38']}\n",
    "intersect = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bed(source, name):\n",
    "    buf = []\n",
    "    for chrm in source[name]:\n",
    "        beds = source[name][chrm].indices[source[name][chrm].data != 0]\n",
    "        data = source[name][chrm].data[source[name][chrm].data != 0].astype(float).astype(int)\n",
    "        ends = np.append(source[name][chrm].indices[1:], \n",
    "              [source[name][chrm].shape])[source[name][chrm].data != 0]\n",
    "\n",
    "        buf.extend([[chrm, beds[i], ends[i], data[i]] for i in range(len(beds))])\n",
    "    buf = np.array(buf)\n",
    "    df = pd.DataFrame([buf[:, 0], buf[:, 1], buf[:, 2], '', buf[:, 3], \"\", buf[:, 1], buf[:, 2], \"\"]).T\n",
    "    df[3] = ''\n",
    "    df[5] = \"+\"\n",
    "    df[8] = ''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = 'hg38'\n",
    "\n",
    "for name in tqdm([i for i in DNA_features_d[gen] if i.upper() in intersect and 'TFs and others' in i]):\n",
    "    df = to_bed(DNA_features_d[gen], name)\n",
    "    df.to_csv(f'tmp/{gen}_{name}.bed', sep = '\\t', index=False, header=None)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(DNA_features_d, name, gen):\n",
    "    df = to_bed(DNA_features_d, name)\n",
    "    df.to_csv(f'tmp/{gen}_{name}.bed', sep = '\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "gen = 'hg38'\n",
    "Parallel(n_jobs = -1)(delayed(func)(DNA_features_d[gen], name, gen)\n",
    "                     for name in tqdm([i for i in DNA_features_d[gen] \n",
    "                                       if i.upper() in intersect and 'TFs and others' in i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = 'hg38'\n",
    "ASSEMBLY = ASSEMBLY_d[mode]\n",
    "chroms = chroms_d[mode]\n",
    "all_features = all_features_d[mode]\n",
    "groups = groups_d[mode]\n",
    "feature_names = feature_names_d[mode]\n",
    "ZDNA = ZDNA_d[mode]\n",
    "black_list = black_list_d[mode]\n",
    "DNA = DNA_d[mode]\n",
    "DNA_features = DNA_features_d[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486243/486243 [00:36<00:00, 13195.16it/s]\n",
      "100%|██████████| 473034/473034 [00:35<00:00, 13186.83it/s]\n",
      "100%|██████████| 387296/387296 [00:26<00:00, 14382.31it/s]\n",
      "100%|██████████| 371512/371512 [00:26<00:00, 14059.15it/s]\n",
      "100%|██████████| 354566/354566 [00:25<00:00, 14128.07it/s]\n",
      "100%|██████████| 333605/333605 [00:26<00:00, 12406.92it/s]\n",
      "100%|██████████| 311222/311222 [00:24<00:00, 12771.62it/s]\n",
      "100%|██████████| 283473/283473 [00:22<00:00, 12419.92it/s]\n",
      "100%|██████████| 270302/270302 [00:20<00:00, 13423.34it/s]\n",
      "100%|██████████| 261323/261323 [00:19<00:00, 13275.77it/s]\n",
      "100%|██████████| 263841/263841 [00:17<00:00, 15138.42it/s]\n",
      "100%|██████████| 260303/260303 [00:20<00:00, 13014.01it/s]\n",
      "100%|██████████| 223367/223367 [00:15<00:00, 14610.85it/s]\n",
      "100%|██████████| 209069/209069 [00:13<00:00, 15231.35it/s]\n",
      "100%|██████████| 199201/199201 [00:13<00:00, 14965.19it/s]\n",
      "100%|██████████| 176442/176442 [00:11<00:00, 14931.76it/s]\n",
      "100%|██████████| 162612/162612 [00:14<00:00, 11550.79it/s]\n",
      "100%|██████████| 156979/156979 [00:11<00:00, 13089.44it/s]\n",
      "100%|██████████| 114487/114487 [00:08<00:00, 12921.34it/s]\n",
      "100%|██████████| 125867/125867 [00:09<00:00, 12982.59it/s]\n",
      "100%|██████████| 91230/91230 [00:06<00:00, 13083.98it/s]\n",
      "100%|██████████| 99254/99254 [00:06<00:00, 14497.46it/s]\n",
      "100%|██████████| 304767/304767 [00:23<00:00, 12936.76it/s]\n",
      "100%|██████████| 111772/111772 [00:06<00:00, 16541.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30088\n",
      "5499166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "width = 512\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "ints_in = []\n",
    "ints_out = []\n",
    "\n",
    "for chrm in chroms:\n",
    "    for st in trange(0, ZDNA[chrm].shape - width, width):\n",
    "        interval = [st, min(st + width, ZDNA[chrm].shape)]\n",
    "        N_count = sum([bp == \"N\" for bp in DNA[chrm][interval[0]:interval[1]]])\n",
    "        bl_count = black_list[chrm][interval[0]:interval[1]].sum()\n",
    "        if N_count > width / 2 or bl_count > 0:\n",
    "            continue\n",
    "        else:\n",
    "            if ZDNA[chrm][interval[0]: interval[1]].any():\n",
    "                ints_in.append([chrm, int(interval[0]), int(interval[1]), 1])\n",
    "            else:\n",
    "                ints_out.append([chrm, int(interval[0]), int(interval[1]), 0])\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "print(len(ints_in))\n",
    "print(len(ints_out))\n",
    "\n",
    "ints_in_full = ints_in\n",
    "ints_out_full = ints_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30088\n",
      "60176\n"
     ]
    }
   ],
   "source": [
    "ints_in = ints_in_full\n",
    "ints_out = [ints_out_full[i] for i in np.random.choice(range(len(ints_out_full)), \n",
    "                                                    size=len(ints_in) * 2, replace=False)]\n",
    "# ints_out = ints_out_full\n",
    "\n",
    "print(len(ints_in))\n",
    "print(len(ints_out))\n",
    "#484 for len 1000 \n",
    "#9680 for len 1000\n",
    "\n",
    "#629 for len 512\n",
    "#12580 for len 5121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "equalized = ints_in + ints_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "divisions = next(StratifiedKFold(5, shuffle=True, \n",
    "                                 random_state=42).split(equalized, [f\"{elem[3]}_{elem[0]}\"\n",
    "                                         for i, elem \n",
    "                                         in enumerate(equalized)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump([equalized, divisions], 'hg_divisions_kouzine.pkl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2kmer(seq, k):\n",
    "    \"\"\"\n",
    "    Convert original sequence to kmers\n",
    "    \n",
    "    Arguments:\n",
    "    seq -- str, original sequence.\n",
    "    k -- int, kmer of length k specified.\n",
    "    \n",
    "    Returns:\n",
    "    kmers -- str, kmers separated by space\n",
    "    \"\"\"\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    kmers = \" \".join(kmer)\n",
    "    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, chroms, features, \n",
    "                 dna_source, features_source, \n",
    "                 labels_source, intervals, tokenizer):\n",
    "\n",
    "        self.chroms = chroms\n",
    "        self.features = features\n",
    "        self.dna_source = dna_source\n",
    "        self.features_source = features_source\n",
    "        self.labels_source = labels_source\n",
    "        self.intervals = intervals\n",
    "        self.le = LabelBinarizer().fit(np.array([[\"A\"], [\"C\"], [\"T\"], [\"G\"]]))\n",
    "        self.configs = {\n",
    "                        'ZHUNT_AS': {\n",
    "                                'CG': 0, 'GC': 1, 'CA': 0, 'AC': 1, \n",
    "                                'TG': 0, 'GT': 1, 'TA': 1, 'AT': 1, \n",
    "                                'CC': 0, 'GG': 0, 'CT': 1, 'TC': 1, \n",
    "                                'GA': 1, 'AG': 1, 'AA': 1, 'TT': 1},\n",
    "                       }\n",
    "        seqs = ([\"A\", \"C\", \"T\", \"G\"] + \n",
    "                ['AC', 'AT', 'AG', 'CT', 'CG', 'GT'] +\n",
    "                ['AAC', 'ACC', 'AAT', 'ATT', 'AAG', 'AGG', \n",
    "                 'CCA', 'CAA', 'CCT', 'CTT', 'CCG', 'CGG', \n",
    "                 'TTA', 'TAA', 'TTC', 'TCC', 'TTG', 'TGG', \n",
    "                 'GGA', 'GAA', 'GGC', 'GCC', 'GGT', 'GTT'] +\n",
    "                ['AAAC', 'AAAT', 'AAAG', 'CCCA', 'CCCT', 'CCCG',\n",
    "                 'TTTA', 'TTTC', 'TTTG', 'GGGA', 'GGGC', 'GGGT'])\n",
    "        self.tars = np.array([self.le.transform(list(i * 11)[:11]) for i in seqs])[:, ::-1, ::-1]\n",
    "        # purine-pyrimidine\n",
    "        self.tars = np.concatenate((self.tars, np.array([self.tars[4] + self.tars[9]])))\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        interval = self.intervals[index]\n",
    "        chrom = interval[0]\n",
    "        begin = int(interval[1])\n",
    "        end = int(interval[2])\n",
    "        ll = list(self.dna_source[chrom][begin:end].upper())\n",
    "        y = self.labels_source[interval[0]][interval[1]: interval[2]]        \n",
    "        \n",
    "        \n",
    "#         DNA PART\n",
    "        \n",
    "        dna_OHE = self.le.transform(ll)[None]\n",
    "        \n",
    "        res = pd.DataFrame(convolve(dna_OHE, self.tars)[:, 5:-5, 3].T / 11)\n",
    "        res = (res.rolling(5, min_periods=1).max().values == 1).astype(int)\n",
    "        \n",
    "        \n",
    "#         ZHUNT PART\n",
    "        zhunts = []\n",
    "        for key in self.configs:\n",
    "            vec = np.array(ll)\n",
    "            vec = np.vectorize(lambda x:self.configs[key].get(x, 0))(\n",
    "                                    np.char.add(vec[1:], vec[:-1]))\n",
    "            zhunts.append(np.concatenate([vec, [0]]))\n",
    "        \n",
    "        \n",
    "        # FEATURES PART\n",
    "        feature_matr = []\n",
    "        for feature in self.features:\n",
    "            source = self.features_source[feature]\n",
    "            feature_matr.append(source[chrom][begin:end])\n",
    "        \n",
    "        # UNION\n",
    "        if len(feature_matr) > 0:\n",
    "            X = np.hstack((\n",
    "                           res,\n",
    "                           np.array(zhunts).T, \n",
    "                           np.array(feature_matr).T/1000)).astype(np.float32)\n",
    "#             X = (np.array(feature_matr).T/1000).astype(np.float32)\n",
    "        else:\n",
    "            X = dna_OHE.astype(np.float32)\n",
    "        \n",
    "        #K-mer part\n",
    "        \n",
    "        k_mers = seq2kmer(self.dna_source[chrom][begin:end+5].upper(),6)\n",
    "        encoded_k_mers = self.tokenizer.encode_plus(k_mers, add_special_tokens=False, max_length=512)[\"input_ids\"]\n",
    "\n",
    "        return torch.Tensor(X), torch.Tensor(y).long(), ll, torch.LongTensor(encoded_k_mers), (chrom, begin, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertConfig, PreTrainedTokenizer, BasicTokenizer, BertForTokenClassification\n",
    "import collections\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\"vocab_file\": {\"dna3\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-3/vocab.txt\",\n",
    "                                             \"dna4\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-4/vocab.txt\",\n",
    "                                             \"dna5\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-5/vocab.txt\",\n",
    "                                             \"dna6\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt\"}}\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "                                          \"dna3\": 512,\n",
    "                                          \"dna4\": 512,\n",
    "                                          \"dna5\": 512,\n",
    "                                          \"dna6\": 512}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"dna3\": {\"do_lower_case\": False},\n",
    "    \"dna4\": {\"do_lower_case\": False},\n",
    "    \"dna5\": {\"do_lower_case\": False},\n",
    "    \"dna6\": {\"do_lower_case\": False}}\n",
    "\n",
    "VOCAB_KMER = {\n",
    "    \"69\": \"3\",\n",
    "    \"261\": \"4\",\n",
    "    \"1029\": \"5\",\n",
    "    \"4101\": \"6\",}\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "class DNATokenizer(PreTrainedTokenizer):\n",
    "    r\"\"\"\n",
    "    Constructs a BertTokenizer.\n",
    "    :class:`~transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n",
    "    Args:\n",
    "        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
    "        do_lower_case: Whether to lower case the input. Only has an effect when do_basic_tokenize=True\n",
    "        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
    "        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n",
    "            minimum of this value (if specified) and the underlying BERT model's sequence length.\n",
    "        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n",
    "            do_basic_tokenize=True\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        do_lower_case=False,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        tokenize_chinese_chars=True,\n",
    "        max_len = 512,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Constructs a BertTokenizer.\n",
    "        Args:\n",
    "            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n",
    "            **do_lower_case**: (`optional`) boolean (default True)\n",
    "                Whether to lower case the input\n",
    "                Only has an effect when do_basic_tokenize=True\n",
    "            **do_basic_tokenize**: (`optional`) boolean (default True)\n",
    "                Whether to do basic tokenization before wordpiece.\n",
    "            **never_split**: (`optional`) list of string\n",
    "                List of tokens which will never be split during tokenization.\n",
    "                Only has an effect when do_basic_tokenize=True\n",
    "            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
    "                Whether to tokenize Chinese characters.\n",
    "                This should likely be deactivated for Japanese:\n",
    "                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.max_len = max_len\n",
    "        #self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n",
    "        #self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file)\n",
    "            )\n",
    "        \n",
    "        self.kmer = VOCAB_KMER[str(len(self.vocab))]\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.basic_tokenizer = BasicTokenizer(\n",
    "                do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
    "                split_tokens.append(token)\n",
    "        # print(split_tokens)\n",
    "        return split_tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A BERT sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            if len(token_ids_0) < 510:\n",
    "                return cls + token_ids_0 + sep\n",
    "            else:\n",
    "                output = []\n",
    "                num_pieces = int(len(token_ids_0)//510) + 1\n",
    "                for i in range(num_pieces):\n",
    "                    output.extend(cls + token_ids_0[510*i:min(len(token_ids_0), 510*(i+1))] + sep)\n",
    "                return output\n",
    "\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        \n",
    "        if len(token_ids_0) < 510:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        else:\n",
    "            output = []\n",
    "            num_pieces = int(len(token_ids_0)//510) + 1\n",
    "            for i in range(num_pieces):\n",
    "                output.extend([1] + ([0] * (min(len(token_ids_0), 510*(i+1))-510*i)) + [1])\n",
    "            return output\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A BERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            if len(token_ids_0) < 510:\n",
    "                return len(cls + token_ids_0 + sep) * [0]\n",
    "            else:\n",
    "                num_pieces = int(len(token_ids_0)//510) + 1\n",
    "                return (len(cls + token_ids_0 + sep) + 2*(num_pieces-1)) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, vocab_path):\n",
    "        \"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"\n",
    "        index = 0\n",
    "        if os.path.isdir(vocab_path):\n",
    "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "        else:\n",
    "            vocab_file = vocab_path\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T11:20:01.394701Z",
     "iopub.status.busy": "2025-04-27T11:20:01.394187Z",
     "iopub.status.idle": "2025-04-27T11:20:18.986330Z",
     "shell.execute_reply": "2025-04-27T11:20:18.985293Z",
     "shell.execute_reply.started": "2025-04-27T11:20:01.394676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=10sF8Ywktd96HqAL0CwvlZZUUGj05CGk5\n",
      "To: /kaggle/working/config.json\n",
      "100%|██████████████████████████████████████████| 634/634 [00:00<00:00, 2.12MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16bT7HDv71aRwyh3gBUbKwign1mtyLD2d\n",
      "To: /kaggle/working/special_tokens_map.json\n",
      "100%|███████████████████████████████████████████| 112/112 [00:00<00:00, 581kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EE9goZ2JRSD8UTx501q71lGCk-CK3kqG\n",
      "To: /kaggle/working/tokenizer_config.json\n",
      "100%|█████████████████████████████████████████| 40.0/40.0 [00:00<00:00, 202kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1gZZdtAoDnDiLQqjQfGyuwt268Pe5sXW0\n",
      "To: /kaggle/working/vocab.txt\n",
      "100%|██████████████████████████████████████| 28.7k/28.7k [00:00<00:00, 57.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "'''!gdown 10sF8Ywktd96HqAL0CwvlZZUUGj05CGk5\n",
    "!gdown 16bT7HDv71aRwyh3gBUbKwign1mtyLD2d\n",
    "!gdown 1EE9goZ2JRSD8UTx501q71lGCk-CK3kqG\n",
    "!gdown 1gZZdtAoDnDiLQqjQfGyuwt268Pe5sXW0\n",
    "\n",
    "\n",
    "!mkdir 6-new-12w-0\n",
    "!mv config.json 6-new-12w-0/\n",
    "!mv special_tokens_map.json 6-new-12w-0/\n",
    "!mv tokenizer_config.json 6-new-12w-0/\n",
    "!mv vocab.txt 6-new-12w-0/'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "train_inds, test_inds = divisions\n",
    "train_intervals, test_intervals = [equalized[i] for i in train_inds], [equalized[i] for i in test_inds]\n",
    "\n",
    "random.shuffle(train_intervals)\n",
    "random.shuffle(test_intervals)\n",
    "\n",
    "train_dataset = Dataset(chroms, \n",
    "                    [i for i in feature_names if i.upper() in intersect], \n",
    "                   DNA, DNA_features, ZDNA, train_intervals, \n",
    "                    BertTokenizer.from_pretrained('6-new-12w-0/'))\n",
    "\n",
    "test_dataset = Dataset(chroms, \n",
    "                   [i for i in feature_names if i.upper() in intersect], \n",
    "                   DNA, DNA_features, ZDNA, test_intervals,\n",
    "                      BertTokenizer.from_pretrained('6-new-12w-0/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        features, labels, sequences, input_ids, intervals = batch            \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #print(model.device, input_ids.device, labels.device)\n",
    "        outputs = model(input_ids = input_ids, labels = labels)        \n",
    "        #print(outputs)\n",
    "        loss, tr_logits = outputs['loss'], outputs['logits']\n",
    "        #print(model(input_ids=ids, attention_mask=mask, labels=labels))\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if (idx + 1) % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            clear_output()\n",
    "            print(f\"Training loss on epoch {epoch} per {idx + 1} training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=0.1\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels, eval_scores = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            features, labels, sequences, input_ids, intervals = batch            \n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(input_ids = input_ids, labels = labels)\n",
    "            loss, eval_logits = outputs['loss'], outputs['logits']            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            flattened_scores = active_logits[:,1] - active_logits[:,0]\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            eval_scores.extend(flattened_scores)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [id.item() for id in eval_labels]\n",
    "    predictions = [id.item() for id in eval_preds]\n",
    "    scores = [id.item() for id in eval_scores]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at zhihan1996/DNA_bert_6 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 88601858 || all params: 88601858 || trainable%: 100.0\n",
      "trainable params: 0 || all params: 88601858 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained('config.json')\n",
    "model = BertForTokenClassification.from_pretrained(\"zhihan1996/DNA_bert_6\", config=config)\n",
    "print_trainable_parameters(model)\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99   9073846\n",
      "           1       0.66      0.42      0.51    169290\n",
      "\n",
      "    accuracy                           0.99   9243136\n",
      "   macro avg       0.82      0.71      0.75   9243136\n",
      "weighted avg       0.98      0.99      0.98   9243136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config_lora = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"TOKEN_CLS\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "device = 0\n",
    "lr = 1e-5\n",
    "EPOCHS = 3\n",
    "\n",
    "dir_to_pretrained_model = \"6-new-12w-0/\"\n",
    "tokenizer = BertTokenizer.from_pretrained('6-new-12w-0/')\n",
    "\n",
    "training_loader = DataLoader(train_dataset, batch_size=24, num_workers = 2)\n",
    "testing_loader = DataLoader(test_dataset, batch_size=16, num_workers = 2)\n",
    "\n",
    "\n",
    "model = get_peft_model(model, config_lora)\n",
    "model.to(device)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(training_loader), epochs=EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training  epoch: {epoch + 1}\")\n",
    "    train(epoch)\n",
    "    labels, predictions, scores = valid(model, testing_loader)\n",
    "    print(f'Validation ROC-AUC: ', roc_auc_score(labels, scores))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7075755,
     "sourceId": 11312858,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7260629,
     "sourceId": 11579945,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [anaconda3-zdna_env]",
   "language": "python",
   "name": "conda-env-anaconda3-zdna_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
